# Story 5.5: Validate New Lambda Readiness

**Status:** Draft

---

## Quick Project Assessment

**Current System Context**
- [x] Legacy Lambda is no longer running; validation relies on golden datasets and comparison tooling captured during earlier extraction work (`docs/brownfield-architecture.md:1683`).
- [x] Comparison monitoring dashboards, alarms, and alerts remain in place to support the deployment decision checkpoints (`docs/epics/epic-5-deployment.md:225`).
- [x] Cutover readiness still demands proof that the new Lambda matches legacy behaviour and operates within performance/error budgets (`docs/prd.md:279`, `docs/epics/epic-5-deployment.md:267`).
- [x] Diff-driven analysis produces structured mismatch reports across SMS, DynamoDB, and Telegram outputs for rapid investigation (`tests/comparison/diff_reporter.py:29`).

**Change Scope**
- [x] Execute an intensive validation campaign (automated regression suite, comparison replays, targeted dry-runs) to demonstrate parity without a live parallel deployment (`docs/brownfield-architecture.md:1687`).
- [x] Investigate and resolve any discrepancies surfaced by comparison telemetry, using structured diff artifacts to confirm fixes (`tests/comparison/diff_reporter.py:55`).
- [x] Maintain rollback readiness during validation drills and document compliance with the <15 minute SLA (`docs/epics/epic-5-deployment.md:314`).
- [x] Package evidence (test reports, dashboards, alarm transcripts, approvals) for the readiness review and go/no-go decision (`docs/epics/epic-5-deployment.md:268`).

---

## Story

**As a** reliability lead,  
**I want** to validate the new Lambda against golden datasets and monitoring checkpoints,  
**so that** stakeholders can approve production cutover with confidence in functional parity.

---

## Story Context

- The go/no-go decision still requires proof of zero discrepancies and healthy performance before traffic moves to the new Lambda (`docs/epics/epic-5-deployment.md:267`).
- Migration success criteria in the PRD require functional parity across SMS, DynamoDB, and Telegram outputs (`docs/prd.md:279`).
- Zero-downtime and rollback expectations remain in force throughout validation, so every issue must be recoverable within minutes (`docs/prd.md:267`).
- Legacy outputs preserved in fixtures enable validation even though the old Lambda is offline (`docs/brownfield-architecture.md:1687`).

---

## Acceptance Criteria

**Functional Requirements**
1. Automated regression suite, comparison replays, and targeted manual dry-runs complete successfully with 100% parity across SMS, DynamoDB, and Telegram outputs; any discrepancies are documented with root-cause notes and resolved prior to sign-off (`docs/brownfield-architecture.md:1687`, `tests/comparison/diff_reporter.py:29`).
2. Comparison artifacts (JSON + markdown) are generated for each validation batch and archived with timestamps for auditability (`tests/comparison/diff_reporter.py:29`).
3. Aggregated validation summary highlights match percentage, error counts, and performance trends needed for the go/no-go review (`docs/epics/epic-5-deployment.md:225`).

**Integration Requirements**
4. CloudWatch dashboards capture validation metrics at the beginning and end of the campaign, with exported evidence attached to the story record (`docs/epics/epic-5-deployment.md:305`).
5. Any comparison-related alarms triggered during testing are triaged within the documented rollback SLA, with outcome notes captured for each incident (`docs/epics/epic-5-deployment.md:284`, `docs/epic-5-deployment.md:324`).
6. Validation evidence (test reports, metrics exports, alarm states, approvals) is appended to `VALIDATION.md` and linked for the readiness review (`docs/epics/epic-5-deployment.md:268`, `VALIDATION.md:1`).

**Quality Requirements**
7. `docs/ops/runbook.md` includes an updated validation playbook describing regression steps, alarm monitoring, and discrepancy escalation paths (`docs/ops/runbook.md:1`).
8. Every deviation or discrepancy investigation receives a completed diff reporter package and remediation summary before validation closes (`tests/comparison/diff_reporter.py:55`).
9. Final readiness report aligns validation outcomes with the PRDâ€™s functional parity success criteria to confirm MSC1 is satisfied prior to cutover approval (`docs/prd.md:279`).

---

## Tasks / Subtasks

- [ ] Prepare validation environment prerequisites (test data snapshots, tooling versions, Secrets Manager roles) and bootstrap the diff reporter output directory so the campaign starts from a clean, reproducible baseline (AC 1, AC 2, AC 3).
- [ ] Execute automated regression suite, comparison replays, and targeted manual dry-runs; log outcomes, metrics, and remediation notes (AC 1, AC 3).
  - [ ] Export CloudWatch dashboard snapshots and metric reports at the start/end of validation for inclusion in the dossier (AC 4, AC 6).
  - [ ] Document SENS/Telegram quota thresholds, establish throttling alert channels, and note fallback handling updates that will be reflected in the ops runbook (AC 4, AC 5, AC 7).
- [ ] Use the diff reporter to analyse any detected mismatches, capture artifacts, and confirm parity after remediation (AC 1, AC 2, AC 8).
- [ ] Track comparison alarms and throttle alerts, documenting response timelines and rehearsing both the <15 minute rollback and rate-limit recovery workflow when alerts stay red past the SLA (AC 4, AC 5).
- [ ] Update `VALIDATION.md` with validation summaries, alarm transcripts, and final readiness endorsements, explicitly noting that runbook/cloudwatch updates include the API quota procedures and attaching evidence of stakeholder/training preparation (AC 3, AC 6, AC 7, AC 9).
  - [ ] Extend `docs/ops/runbook.md` and `docs/ops/cloudwatch-queries.md` with validation procedures, saved queries, and the newly defined quota mitigation steps (AC 7).
- [ ] Draft stakeholder communication, escalation, and enablement plans; schedule briefing/training sessions and capture sign-off artifacts for the readiness dossier (AC 3, AC 9).
- [ ] Facilitate the readiness review meeting and attach the sign-off artifacts that show MSC1 parity achieved (AC 3, AC 9).

---

## Dev Notes

- Rely on the comparison diff tooling to produce structured mismatch reports and avoid ad-hoc analysis (`tests/comparison/diff_reporter.py:29`).
- Use CloudWatch dashboards and existing alarm playbooks to detect anomalies quickly during validation (`docs/epics/epic-5-deployment.md:225`, `docs/ops/runbook.md:5`).
- Keep rollback runbooks handy so the validation team can disable the new Lambda within minutes if parity breaks (`docs/epic-5-deployment.md:324`).
- Capture all telemetry evidence and approvals promptly to streamline the readiness review (`docs/epics/epic-5-deployment.md:268`).

### Testing

- Verify comparison metrics after each validation batch via `aws cloudwatch get-metric-statistics` (or dashboard widgets) to confirm match percentage remains 100% (`docs/epics/epic-5-deployment.md:225`).
- Execute diff reporter checks against sampled runs to ensure archived artifacts remain mismatch-free (`tests/comparison/diff_reporter.py:55`).
- Simulate at least one alarm transition to validate response time and rollback readiness within the <15 minute SLA (`docs/epic-5-deployment.md:324`).
- Dry-run the readiness briefing using the assembled evidence pack to confirm all MSC1 checkpoints are covered (`docs/prd.md:279`).

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-19 | 0.1 | Story rewritten to reflect PO directive (no parallel run) | Sarah (PO) |

---

## Dev Agent Record

### Agent Model Used
- Pending assignment

### Debug Log References
- Pending implementation

### Completion Notes
- No development activity recorded yet; update upon implementation.

### File List
- Pending updates

---

## QA Results

### Review Date
- Pending review

### Reviewed By
- Pending assignment

### Summary
- QA evaluation not yet performed; populate after readiness review.
