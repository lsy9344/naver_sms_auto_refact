# Story 3.5: Unit Tests for Rule Engine

**Status:** Draft

---

## Quick Project Assessment

**Current System Context**
- [x] Rule engine core (Story 3.1), condition evaluators (Story 3.2), action executors (Story 3.3), and rules configuration (Story 3.4) drafted.
- [x] Testing infrastructure plan outlined in architecture doc (`docs/brownfield-architecture.md:1600-1685`).
- [x] Regression goals defined in PRD Section 4.1 and validation checklist in `VALIDATION.md`.
- [x] Existing deficits: zero automated coverage for legacy logic, manual production validation only.

**Change Scope**
- [x] Build comprehensive unit and integration test suites covering rule evaluation, conditions, actions, and regression comparisons.
- [x] Provide fixtures, factories, and utilities to keep future tests maintainable.
- [x] Success measured by >80% coverage for rule engine package and green regression comparisons against legacy outputs.

---

## Story

**As a** QA-focused developer,
**I want** automated tests for the rule engine that prove behavioral parity with the legacy system,
**so that** refactoring delivers confidence and guards against regressions.

---

## Story Context

**Existing System Integration:**
- Integrates with: `tests/` directory structure (unit, integration, regression), CI pipeline.
- Technology: pytest, pytest-asyncio, moto/localstack for AWS mocks, optional golden file comparisons.
- Touch points: `src/rules/`, `src/database/`, `src/notifications/`, `config/rules.yaml`, `VALIDATION.md`.

---

## Acceptance Criteria

**Functional Requirements**
1. Unit tests cover each rule engine component:
   - RuleEngine core: rule loading, condition evaluation, action execution, error handling, result aggregation.
   - Condition evaluators: true/false scenarios, missing context, edge cases (e.g., timezone boundaries).
   - Action executors: success paths with mocked dependencies, error propagation, logging.
2. Regression test suite (`tests/regression/test_rule_engine_parity.py`) runs legacy logic (captured via fixture data) and new rule engine side-by-side, asserting identical outputs for SMS sends, DB updates, and notifications.
3. Coverage report demonstrates â‰¥80% line coverage for `src/rules/` package (`pytest --cov=src/rules`), with summary appended to `VALIDATION.md`.

**Integration Requirements**
4. Integration tests (`tests/integration/test_rule_engine_e2e.py`) run the full pipeline with local DynamoDB/SNS stubs to ensure rule execution triggers expected side effects.
5. Test fixtures include:
   - Booking payload samples extracted from production logs (sanitized).
   - DynamoDB state fixtures before/after rule execution.
   - SMS/notification expectation fixtures (JSON).
6. CI workflow (`.github/workflows/test-rule-engine.yml` or equivalent) executes unit + regression suites on pull requests, failing on coverage regression or parity mismatch.

**Quality Requirements**
7. Test documentation (`docs/testing/rule-engine-tests.md`) explains structure, fixtures, commands, and how to update golden files.
8. Regression harness produces diff-friendly output when mismatches occur (e.g., JSON file per booking); sample failure output documented.
9. Linting/static analysis ensures tests avoid hitting real services (guard rails preventing live HTTP calls when environment variables absent).

---

## Technical Notes

- **Tools:** pytest, pytest-cov, pytest-asyncio, moto/localstack, freezegun for time control.
- **Fixtures:** Provide factory helpers in `tests/factories/booking_factory.py` to replicate booking data; use `load_legacy_snapshot()` to import historical results.
- **Validation:** After initial pass, run tests against staging data and capture results in `VALIDATION.md` with timestamp and commit hash.

---

## Definition of Done

- [ ] Unit, integration, regression tests implemented and passing
- [ ] Coverage threshold met and enforced
- [ ] Documentation/fixtures maintained
- [ ] CI pipeline updated
- [ ] Validation evidence recorded

---

## Risk and Compatibility Check

- **Primary Risk:** Flaky tests due to time dependence or live AWS calls.
- **Mitigation:** Freeze time, mock external services, include deterministic fixtures.
- **Rollback:** If regression harness blocks release with false positives, toggle via environment variable while investigating (documented SOP).

**Compatibility Verification**
- [x] Tests support future rule additions  
- [x] Re-usable fixtures for other epics (integration testing)  
- [x] Aligns with QA gate expectations

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-18 | 1.0 | Story drafted from Epic 3 requirements | Sarah (PO) |

