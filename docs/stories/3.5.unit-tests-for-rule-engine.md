# Story 3.5: Unit Tests for Rule Engine

**Status:** Ready for review

---

## Quick Project Assessment

**Current System Context**
- [x] Rule engine core (Story 3.1), condition evaluators (Story 3.2), action executors (Story 3.3), and rules configuration (Story 3.4) drafted.
- [x] Testing infrastructure plan outlined in architecture doc (`docs/brownfield-architecture.md:1600-1685`).
- [x] Regression goals defined in PRD Section 4.1 and validation checklist in `VALIDATION.md`.
- [x] Existing deficits: zero automated coverage for legacy logic, manual production validation only.

**Change Scope**
- [x] Build comprehensive unit and integration test suites covering rule evaluation, conditions, actions, and regression comparisons.
- [x] Provide fixtures, factories, and utilities to keep future tests maintainable.
- [x] Success measured by >80% coverage for rule engine package and green regression comparisons against legacy outputs.

---

## Story

**As a** QA-focused developer,
**I want** automated tests for the rule engine that prove behavioral parity with the legacy system,
**so that** refactoring delivers confidence and guards against regressions.

---

## Story Context

**Existing System Integration:**
- Integrates with: `tests/` directory structure (unit, integration, regression), CI pipeline.
- Technology: pytest, pytest-asyncio, moto/localstack for AWS mocks, optional golden file comparisons.
- Touch points: `src/rules/`, `src/database/`, `src/notifications/`, `config/rules.yaml`, `VALIDATION.md`.

---

## Acceptance Criteria

**Functional Requirements**
1. Unit tests cover each rule engine component:
   - RuleEngine core: rule loading, condition evaluation, action execution, error handling, result aggregation.
   - Condition evaluators: true/false scenarios, missing context, edge cases (e.g., timezone boundaries).
   - Action executors: success paths with mocked dependencies, error propagation, logging.
2. Regression test suite (`tests/regression/test_rule_engine_parity.py`) runs legacy logic (captured via fixture data) and new rule engine side-by-side, asserting identical outputs for SMS sends, DB updates, and notifications.
3. Coverage report demonstrates ≥80% line coverage for `src/rules/` package (`pytest --cov=src/rules`), with summary appended to `VALIDATION.md`.

**Integration Requirements**
4. Integration tests (`tests/integration/test_rule_engine_e2e.py`) run the full pipeline with local DynamoDB/SNS stubs to ensure rule execution triggers expected side effects.
5. Test fixtures include:
   - Booking payload samples extracted from production logs (sanitized).
   - DynamoDB state fixtures before/after rule execution.
   - SMS/notification expectation fixtures (JSON).
6. CI workflow (`.github/workflows/test.yml`) integrates rule engine tests under new "rule-engine-tests" job that executes on PR and push events. Job runs unit + regression suites, failing on coverage drop below 80% or regression assertion failures.

**Quality Requirements**
7. Test documentation (`docs/testing/rule-engine-tests.md`) explains structure, fixtures, commands, and how to update golden files.
8. Regression harness produces diff-friendly output when mismatches occur (e.g., JSON file per booking); sample failure output documented.
9. Bandit security scanning (via `.bandit` config) detects direct HTTP calls (requests, urllib) when AWS_PROFILE or MOCK_AWS environment variables are absent. Specific rules enforced: B301 (pickle), B302 (tempfile), B303 (unverified SSL). Pre-commit hook enforces; CI blocks PRs with violations.

---

## Dev Notes

### Testing Infrastructure Context
**Reference:** `docs/brownfield-architecture.md` Lines 1600-1685 (Testing Infrastructure Plan)

- **Test directory structure:** `tests/{unit,integration,regression}` for organized test categorization
- **Framework:** pytest with pytest-cov for coverage reporting; pytest-asyncio for async code paths
- **AWS mocking:** moto library for mocking DynamoDB, SNS, SQS services (no real AWS calls)
- **Time control:** freezegun for deterministic time-based tests (booking deadlines, hour checks)
- **Test naming convention:** `test_{module}_{function_under_test}.py` (e.g., `test_rule_engine_evaluate.py`)

### Source Tree Structure (Relevant to This Story)

**Rule Engine Components:**
- `src/rules/engine.py` - RuleEngine class (Story 3.1)
- `src/rules/conditions/` - Condition evaluators (Story 3.2) - 6 types implemented
- `src/rules/actions/` - Action executors (Story 3.3) - 6 types implemented
- `src/config/rules.yaml` - Rule configuration (Story 3.4)
- `src/config/rules.schema.json` - JSON schema validation (Story 3.4)

**Database & Domain Models:**
- `src/database/` - DynamoDB client, booking table operations
- `src/domain/` - Booking, RuleConfig dataclasses

**Test Infrastructure:**
- `tests/unit/` - Unit tests for individual components
- `tests/integration/` - Integration tests with full pipeline
- `tests/fixtures/` - Test data: `legacy_bookings.json`, `legacy_expected_actions.json`
- `tests/factories/` - Factory helpers: `booking_factory.py` for dynamic test data creation
- `conftest.py` - Shared pytest fixtures and configuration

### Testing Standards (Architecture Doc References)

**Coverage Requirements:**
- Minimum coverage: >80% for `src/rules/` package
- Verify with: `pytest --cov=src/rules --cov-report=term-missing`
- Failed coverage breaks CI pipeline

**Unit Test Patterns:**
- Each condition evaluator: Test with valid/invalid inputs, edge cases (timezone boundaries, null values)
- Each action executor: Test success paths with mocked dependencies, error propagation, logging
- RuleEngine core: Test rule loading, validation, evaluation logic, error handling, result aggregation

**Integration Test Patterns:**
- Full rule evaluation cycle with real context objects
- Integration with mocked DynamoDB/SNS/Telegram services
- Error scenarios: malformed rules, missing config, service failures

**Regression Test Patterns:**
- Load legacy booking fixtures from `tests/fixtures/legacy_bookings.json`
- Process through new rule engine
- Compare action sequences against baseline in `tests/fixtures/legacy_expected_actions.json`
- Assert 100% parity on SMS sends, DB updates, notifications

**Fixture Management:**
- Booking factory at `tests/factories/booking_factory.py` - creates realistic booking objects
- Legacy fixtures versioned in Git at `tests/fixtures/`
- Fixtures anonymized: phone numbers, store IDs sanitized per compliance

### Key Testing Patterns from Stories 3.1-3.4

**Condition Evaluators (Story 3.2):**
- 6 condition types: `booking_not_in_db`, `time_before_booking`, `flag_not_set`, `current_hour`, `booking_status`, `has_option_keyword`
- Each evaluator callable: `evaluate(context: Dict) -> bool`
- Test with mocked database and various context states

**Action Executors (Story 3.3):**
- 6 action types: `send_sms`, `create_db_record`, `update_flag`, `send_telegram`, `send_slack` (template), `log_event`
- Each executor callable: `execute(params: Dict, context: Dict) -> ActionResult`
- Test with mocked SENS/Telegram APIs, mocked DynamoDB

**Rule Engine (Story 3.1):**
- `RuleEngine.evaluate_rule(rule, context) -> bool` - AND logic for all conditions
- `RuleEngine.execute_rule(rule, context) -> None` - Execute all actions in sequence
- `RuleEngine.process_booking(booking) -> List[ActionResult]` - Process through all enabled rules

**Configuration (Story 3.4):**
- Rules loaded from YAML at startup
- Schema validation on load
- 3 active production rules + 2 disabled templates
- Verified via `scripts/print_rules.py`

### Testing

**Test File Structure Expected:**
```
tests/
├── unit/
│   ├── test_rule_engine_core.py       (RuleEngine class methods)
│   ├── test_conditions_*.py           (Each condition evaluator)
│   ├── test_actions_*.py              (Each action executor)
│   └── test_rules_schema.py           (Already exists - AC validation)
├── integration/
│   ├── test_rule_engine_e2e.py        (Full pipeline integration)
│   └── test_rules_regression.py       (Legacy comparison - partial exists)
├── fixtures/
│   ├── legacy_bookings.json           (5 booking scenarios)
│   └── legacy_expected_actions.json   (Expected baselines)
├── factories/
│   ├── __init__.py
│   └── booking_factory.py             (BookingFactory for test data)
└── conftest.py                        (Shared mocking, fixtures)
```

**Local Test Execution Examples:**
- Run all rule engine tests: `pytest tests/unit/test_rule* tests/integration/test_rule* -v`
- Run with coverage report: `pytest tests/ --cov=src/rules --cov-report=html`
- Run regression suite only: `pytest tests/integration/test_rules_regression.py::TestRulesRegression -v`
- Run with time frozen: `FREEZE_TIME=2025-10-19T12:00:00Z pytest tests/`

---

## Technical Notes

- **Tools:** pytest, pytest-cov, pytest-asyncio, moto/localstack, freezegun for time control.
- **Fixtures:** Provide factory helpers in `tests/factories/booking_factory.py` to replicate booking data; use `load_legacy_snapshot()` to import historical results.
- **Validation:** After initial pass, run tests against staging data and capture results in `VALIDATION.md` with timestamp and commit hash.

---

## Definition of Done

- [ ] Unit, integration, regression tests implemented and passing
- [ ] Coverage threshold met and enforced
- [ ] Documentation/fixtures maintained
- [ ] CI pipeline updated
- [ ] Validation evidence recorded

---

## Risk and Compatibility Check

- **Primary Risk:** Flaky tests due to time dependence or live AWS calls.
- **Mitigation:** Freeze time, mock external services, include deterministic fixtures.
- **Rollback:** If regression harness blocks release with false positives, toggle via environment variable while investigating (documented SOP).

**Compatibility Verification**
- [x] Tests support future rule additions  
- [x] Re-usable fixtures for other epics (integration testing)  
- [x] Aligns with QA gate expectations

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-19 | 1.1 | Validation complete: Added Dev Notes, clarified AC6 and AC9, moved to Approved status | Sarah (PO) |
| 2025-10-18 | 1.0 | Story drafted from Epic 3 requirements | Sarah (PO) |

## QA Results

### Review Date: 2025-10-19
### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment
- `tests/integration/test_rules_regression.py:238` only compares the length of actual vs expected actions; it ignores rule/action identity, parameters, and `ActionResult.success`. Any mismatch (or executor failure) still reports success, so AC2’s “identical outputs” guarantee is unenforced.
- `tests/integration/test_rules_regression.py:311` keeps `mock_db_repo.get_booking` returning `None`, which makes `update_flag` raise `ActionExecutionError`. Because the harness ignores the failed results, regression coverage never exercises the happy path for flag updates or action sequencing.
- `.github/workflows/test.yml` is missing; the promised `rule-engine-tests` job (AC6) is not implemented, so coverage and regression enforcement cannot run in CI.

### Test Coverage & Evidence
- Unit suites exist for engine, conditions, and actions; documentation cites 162 tests @88% coverage, but the regression harness defect means parity is unverified despite the reported counts.
- Fixtures (`tests/fixtures/*.json`) and docs (`docs/testing/rule-engine-tests.md`) are present, yet without a working regression assertion and CI run they are insufficient evidence for release.

### Acceptance Criteria Validation
- PASS: AC1, AC3–AC5, AC7–AC8 (structure, fixtures, documentation reviewed manually).
- FAIL: AC2 (regression harness fails to validate outputs), AC6 (CI job absent), AC9 (no `.bandit` configuration or enforcement present).

### Recommendations
- Extend the regression runner to compare each expected action dict (rule name, type, params, success) and surface diffs; update mocks to return realistic booking records so `update_flag` paths execute successfully.
- Add `.github/workflows/test.yml` with a `rule-engine-tests` job running unit + regression suites with `--cov=src/rules --cov-fail-under=80`.
- Introduce the `.bandit` configuration described in AC9 and wire it into pre-commit/CI so HTTP usage without `MOCK_AWS`/`AWS_PROFILE` flags fails the build.

### Suggested Status
- **Changes Required** before the gate can pass. Recommend re-review after harness, CI, and security tasks are completed.
