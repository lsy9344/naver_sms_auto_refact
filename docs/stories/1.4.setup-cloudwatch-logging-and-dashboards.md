# Story 1.4: Setup CloudWatch Logging & Dashboards

**Status:** Draft

---

## Story

**As a** reliability engineer,  
**I want** to configure CloudWatch log groups, metrics, dashboards, and alarms for the Naver SMS automation service,  
**so that** the team can monitor system health, detect failures quickly, and meet the PRD observability requirements.

---

## Acceptance Criteria

**Functional Requirements**
1. CloudWatch Log Group `/aws/lambda/naver-sms-automation` created with retention set to 90 days and encryption enabled (default AWS-managed key acceptable for now).
2. Lambda logging wrapper emits structured JSON logs with correlation fields (`request_id`, `rule_name`, `action_type`, `status`) and masks PII/secret values before writing; sample log output stored in `VALIDATION.md`.
3. Metric filters defined for key events: `sms_sent_total`, `sms_failed_total`, `login_failure_total`, `secrets_error_total`, enabling CloudWatch metrics without modifying downstream code.
4. CloudWatch Dashboard `naver-sms-automation-dashboard` displays:
   - Daily SMS volume by template (stacked bar or line)
   - Error count/timeouts (Lambda errors, throttles)
   - Login success vs failure
   - Recent rule execution latency (p50/p95)
5. CloudWatch Alarms configured for:
   - Any Lambda invocation error >0 in 5-minute window (SNS/email action)
   - Secrets retrieval failures >0 in 15-minute window
   - Login failures >3 in 30 minutes (indicating credential or site issue)
6. Runbook section (`docs/ops/runbook.md`) documents alarm meaning, response steps, and dashboard interpretation.

**Integration Requirements**
7. Lambda container image installs structured logging utility and environment variables (`LOG_LEVEL`, `ENABLE_STRUCTURED_LOGS`) default to project standards.
8. Terraform/CloudFormation definitions created or updated for log group, metric filters, dashboard, and alarms, ensuring reproducible setup tied to Story 1.5.
9. Smoke test (`scripts/emit_sample_logs.py`) writes sample logs/events, and dashboard/metrics confirm correct ingestion (screenshots or metric screenshots attached to `VALIDATION.md`).

**Quality Requirements**
10. CloudWatch Logs Insights query examples documented (`docs/ops/cloudwatch-queries.md`) covering at least: filter by rule, filter by phone suffix, count failures per day.
11. No sensitive data (full phone numbers, credentials) present in logs or dashboards; validation performed via automated redaction unit test.
12. Observability configuration reviewed by QA/PO and accepted, with checklist entry updated to include log/metric verification.

---

## Tasks / Subtasks

- [ ] Provision CloudWatch log infrastructure (AC: 1, 3, 5)
  - [ ] Create log group, retention, encryption
  - [ ] Define metric filters and alarms
- [ ] Implement structured logging (AC: 2, 7, 11)
  - [ ] Update logging helper in `src/utils/logger.py`
  - [ ] Add redaction unit tests
- [ ] Build dashboard and documentation (AC: 4, 6, 10)
  - [ ] Configure dashboard widgets
  - [ ] Update runbook and query docs
- [ ] Validate end-to-end (AC: 8, 9, 12)
  - [ ] Update IaC templates
  - [ ] Execute smoke script and capture evidence
  - [ ] Record QA review outcome

---

## Dev Notes

- **References:**  
  - PRD monitoring requirements (`docs/prd.md:134-160, 307-335`).  
  - Architecture guidance on structured logging (`docs/brownfield-architecture.md:195-205`, `1059-1068`).  
  - Existing log group name in Epic acceptance criteria (`docs/epics/epic-1-infrastructure-setup.md`).
- **Implementation Tips:**  
  - Use Lambda Powertools or custom JSON logger to include correlation IDs.  
  - Metric filters can parse `status` field from JSON logs (`$.status`).  
  - Dashboards can combine metrics (`AWS/Lambda`, custom namespace `NaverSMSAutomation`).
- **Testing Guidance:**  
  - Use `aws logs put-log-events` or sample Lambda invocation to verify metric filters.  
  - Capture CloudWatch console screenshots for acceptance evidence.

---

## Definition of Done

- [ ] Functional requirements satisfied  
- [ ] Dashboards and alarms active  
- [ ] Documentation/runbook updated  
- [ ] Evidence stored in `VALIDATION.md`  
- [ ] QA review completed

---

## Risk and Compatibility Check

- **Primary Risk:** Overly noisy alarms leading to alert fatigue.  
  **Mitigation:** Start with generous thresholds and tune after first week of operations.

- **Rollback Plan:** Disable new metric filters and alarms via IaC rollback; revert logger to plain text (temporary) if structured logging causes issues.

**Compatibility Verification**
- [x] No impact on customer-facing behavior  
- [x] Minimal performance impact (logging overhead acceptable)  
- [x] Works in parallel with existing logging until cutover

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-18 | 1.0 | Story drafted from Epic 1 requirements | Sarah (PO) |

