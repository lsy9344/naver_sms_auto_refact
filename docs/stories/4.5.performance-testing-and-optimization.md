# Story 4.5: Performance Testing & Optimization

**Status:** Approved

---

## Quick Project Assessment

**Current System Context**
- [x] Performance NFRs require Lambda executions under 4 minutes, cold starts under 10 seconds, and DynamoDB latency below 100 ms to protect customer notifications (`docs/prd.md:234-238`).
- [x] Epic 4 mandates performance tests covering 100-booking load, execution profiling, memory checks, and cold-start timing before deployment gates (`docs/epics/epic-4-integration-testing.md:210-247`).
- [x] Structured logging already emits `duration_ms` for operations, enabling precise measurement without extra libraries when properly invoked (`docs/ops/logging.md:129-168`).
- [x] CloudWatch Insights queries for slow requests, duration thresholds, and per-action baselines are defined to monitor regressions in production (`docs/ops/cloudwatch-queries.md:150-200`).
- [x] Architecture analysis highlights Selenium’s global driver initialization as a cold-start bottleneck and DynamoDB scans as a scaling risk that must be validated during load tests (`docs/brownfield-architecture.md:785-921`).

**Change Scope**
- [x] Baseline the containerized Lambda against NFR thresholds using measured execution, cold-start, and memory data gathered from CloudWatch and comparison fixtures (`docs/epics/epic-4-integration-testing.md:242-247`, `docs/prd.md:234-238`).
- [x] Extend the comparison dataset from Story 4.2 into a reusable performance harness that replays ≥100 bookings to validate throughput and surface bottlenecks (`docs/stories/4.2.implement-comparison-testing-framework.md:42-55`, `docs/epics/epic-4-integration-testing.md:242-247`).
- [x] Implement or confirm optimizations that keep Selenium initialization lazy and reduce unnecessary DynamoDB work so measured metrics stay within targets (`docs/brownfield-architecture.md:785-921`).
- [x] Capture monitoring queries, alert thresholds, and validation evidence in project docs so QA/PO can confirm MSC4 performance readiness (`docs/ops/cloudwatch-queries.md:150-206`, `docs/prd.md:299-305`).
- [x] Respect PRD scope boundaries by limiting work to NFR compliance—defer advanced optimizations like rule-engine parallelism unless measurements prove they are required (`docs/prd.md:391-394`).

---

## Story

**As a** performance engineer,  
**I want** to measure and tune the refactored Lambda so it consistently meets our performance NFRs,  
**so that** we can deploy with confidence that customer notifications remain timely and reliable.

---

## Story Context

- Comparison testing already provides sanitized production fixtures and deterministic orchestration we can reuse for load/performance replays without exposing PII (`docs/stories/4.2.implement-comparison-testing-framework.md:42-55`).
- The Dockerized runtime from Story 4.3 standardizes dependencies, making local Lambda RIE runs representative of production for execution-time sampling (`docs/stories/4.3.build-docker-container.md:9-72`).
- NFR performance targets are a hard go/no-go gate for Epic 4 and for the Week-4 deployment milestone; failure risks timeouts and missed SMS/Telegram notifications (`docs/epics/epic-4-integration-testing.md:210-214`, `docs/prd.md:544-551`).
- Architecture notes call out cold-start overhead from Selenium and table scans in DynamoDB; validating mitigation effectiveness is part of this story’s acceptance (`docs/brownfield-architecture.md:785-921`).

---

## Acceptance Criteria

**Functional Requirements**
1. Performance baseline demonstrates execution duration, cold-start, and memory metrics within NFR thresholds across test runs (≤4 min execution, ≤10 s cold start, ≤512 MB memory) using comparison fixtures and CloudWatch measurements (`docs/prd.md:234-238`, `docs/epics/epic-4-integration-testing.md:210-214`).
2. Load/performance suite replays at least 100 bookings end-to-end, records throughput, and surfaces any hotspots while reusing sanitized datasets prepared in Story 4.2 (`docs/epics/epic-4-integration-testing.md:242-245`, `docs/stories/4.2.implement-comparison-testing-framework.md:42-55`).
3. Mitigations for cold-start overhead and avoidable DynamoDB scans are verified—Selenium driver stays lazily initialized and heavy scans are profiled or optimized if they threaten NFR compliance (`docs/brownfield-architecture.md:785-921`, `docs/prd.md:544-551`).

**Integration Requirements**
4. Structured logging captures `duration_ms`, memory, and phase markers for key operations, and CloudWatch Insights queries for slow paths are documented/exercised so on-call teams can monitor regressions (`docs/ops/logging.md:129-168`, `docs/ops/cloudwatch-queries.md:150-200`).
5. Performance validation produces repeatable scripts or commands (Lambda RIE, CloudWatch CLI, pytest markers) and documents how to rerun them during CI or pre-release reviews (`docs/epics/epic-4-integration-testing.md:242-247`, `docs/stories/4.3.build-docker-container.md:109-150`).

**Quality Requirements**
6. Results, thresholds, and any tuning decisions are recorded in `VALIDATION.md`, with explicit note that advanced optimization remains deferred unless NFR breaches persist (`docs/prd.md:299-305`, `docs/prd.md:391-394`).

---

## Tasks / Subtasks

- [ ] **Task 1: Collect Baseline Metrics (AC: 1, 4)**
  - [ ] Run containerized handler via Lambda RIE and AWS CloudWatch CLI commands to capture execution, cold-start, and memory metrics for multiple runs (`docs/stories/4.3.build-docker-container.md:109-150`, `docs/brownfield-architecture.md:1835-1856`).
  - [ ] Ensure structured logging wraps key phases (`load_settings`, `authenticate`, `process_rules`, `send_summary`) with `duration_ms` populated for each segment (`docs/ops/logging.md:129-168`).
  - [ ] Execute CloudWatch Insights queries for slow requests and per-action averages to confirm no operation exceeds thresholds (`docs/ops/cloudwatch-queries.md:150-200`).

- [ ] **Task 2: Build Load & Performance Harness (AC: 2, 5)**
  - [ ] Extend Story 4.2 comparison runner or create `tests/performance/test_lambda_performance.py` to replay ≥100 bookings and record throughput/latency metrics (`docs/stories/4.2.implement-comparison-testing-framework.md:42-55`, `docs/epics/epic-4-integration-testing.md:242-245`).
  - [ ] Tag pytest suite or scripts so CI can execute performance runs on demand without impacting default pipelines (`docs/epics/epic-4-integration-testing.md:242-247`).
  - [ ] Persist anonymized performance outputs (JSON/CSV) under `tests/fixtures/performance/` for regression comparisons (`docs/prd.md:300-304`).

- [ ] **Task 3: Verify Cold-Start & DynamoDB Optimizations (AC: 1, 3)**
  - [ ] Confirm Selenium driver initialization occurs only when cookies are invalid and record cold-start durations before/after adjustments (`docs/brownfield-architecture.md:785-803`).
  - [ ] Profile DynamoDB scans during load runs and document whether additional optimizations or indexing should be queued (`docs/brownfield-architecture.md:908-921`).
  - [ ] If metrics exceed limits, implement minimal changes (e.g., prefetch caching, batch sizing) and re-measure to demonstrate compliance (`docs/prd.md:544-551`, `docs/prd.md:391-394`).

- [ ] **Task 4: Document Monitoring & Alerts (AC: 4, 5)**
  - [ ] Produce step-by-step runbook snippet with exact CloudWatch queries, filters, and thresholds for performance monitoring (`docs/ops/cloudwatch-queries.md:150-200`, `docs/brownfield-architecture.md:1835-1856`).
  - [ ] Outline alert conditions (e.g., duration >240 s, cold-start >10 s) and tie them to existing logging fields so operations can automate alarms (`docs/prd.md:234-238`, `docs/ops/logging.md:129-168`).
  - [ ] Share rerun instructions (Lambda RIE commands, pytest markers, AWS CLI scripts) with QA/PO for go-live verification (`docs/stories/4.3.build-docker-container.md:109-150`, `docs/prd.md:299-305`).

- [ ] **Task 5: Capture Evidence (AC: 6)**
  - [ ] Append performance findings, dataset identifiers, and threshold confirmation to `VALIDATION.md` aligned with MSC4 evidence expectations (`docs/prd.md:299-305`).
  - [ ] Note any optimization work deferred beyond MVP and reference PRD scope boundaries (`docs/prd.md:391-394`).

---

## Dev Notes

### Previous Story Insights
- Story 4.2 delivers sanitized fixtures and deterministic runners; reuse its dataset manifest and masking approach for performance scenarios (`docs/stories/4.2.implement-comparison-testing-framework.md:42-55`). No Dev Agent record exists yet for Story 4.3; status remains Draft (`docs/stories/4.3.build-docker-container.md:3`).

### Performance Baselines
- Enforce execution <4 minutes, cold start <10 seconds, and memory <512 MB when validating runs (`docs/prd.md:234-238`, `docs/epics/epic-4-integration-testing.md:210-214`).
- MSC4 requires documented load-test confirmation before release; failing to supply evidence blocks deployment (`docs/prd.md:299-305`).

### Optimization Targets
- Keep Selenium driver creation lazy so warm invocations avoid the Chrome startup penalty (`docs/brownfield-architecture.md:785-803`).
- Monitor DynamoDB scans (`scan_unnotified_options`) because full-table scans slow as data grows; document any mitigation recommendations (`docs/brownfield-architecture.md:908-921`).
- Respect MVP scope—defer advanced rule-engine parallelism or caching unless thresholds are breached (`docs/prd.md:391-394`).

### Monitoring & Instrumentation
- Apply `logger.info/error(..., duration_ms=...)` or `@log_operation` to capture timings at major phases for later Insights analysis (`docs/ops/logging.md:129-168`).
- Use predefined CloudWatch Insights queries for slowest requests, >5 second operations, and action averages to validate data in production and during tests (`docs/ops/cloudwatch-queries.md:150-200`).
- For CLI-based checks, follow architecture commands to extract execution durations and DynamoDB capacity metrics (`docs/brownfield-architecture.md:1835-1856`).

### Data & Fixtures
- Store new performance results alongside existing fixtures under `tests/fixtures/` with anonymized payloads to stay compliant with masking expectations (`docs/stories/4.2.implement-comparison-testing-framework.md:42-55`, `docs/prd.md:252-257`).
- Ensure output comparisons leverage the same normalization pipeline as parity tests to avoid nondeterministic diffs (`docs/stories/4.2.implement-comparison-testing-framework.md:42-55`).

### Project Structure Alignment
- Follow the planned module layout with `src/main.py` orchestrating auth, API, rule engine, and notifications; ensure any helper utilities or benchmarks respect this structure (`docs/brownfield-architecture.md:935-959`).
- Place new test modules within `tests/` alongside existing unit/integration suites so CI discovery remains consistent (`docs/testing/rule-engine-tests.md:20-44`).

### Testing
- Execute Lambda RIE runs (`docker run --rm -p 9000:8080 --env-file .env naver-sms-automation:latest`) and invoke via curl to simulate live execution (`docs/stories/4.3.build-docker-container.md:109-150`).
- Run pytest with performance markers (e.g., `pytest -m performance`) to exercise the load harness and capture duration metrics (`docs/epics/epic-4-integration-testing.md:242-247`).
- Use AWS CLI commands from architecture guidance to validate execution time and DynamoDB capacity against prod logs before sign-off (`docs/brownfield-architecture.md:1835-1856`).

---

## Definition of Done

- [ ] All acceptance criteria satisfied with documented evidence.
- [ ] Performance harness committed and reproducible in CI/local environments.
- [ ] CloudWatch queries and monitoring instructions updated for operations.
- [ ] `VALIDATION.md` updated with performance metrics, thresholds, and follow-up actions.
- [ ] Checklist `story-draft-checklist` executed and noted with outcomes.

---

## Risk and Compatibility Check

- **Primary Risk:** Performance regressions slipping past validation if load harness is not maintained.  
  **Mitigation:** Keep fixtures updated with recent sanitized data and automate Insights queries in CI smoke tests (`docs/stories/4.2.implement-comparison-testing-framework.md:42-55`, `docs/ops/cloudwatch-queries.md:150-200`).

- **Secondary Risk:** Over-optimizing beyond MVP scope leading to rework or architectural drift.  
  **Mitigation:** Limit changes to those required to meet NFRs and document any future optimization backlog items (`docs/prd.md:391-394`).

**Compatibility Verification**
- [x] Aligns with Dockerized Lambda runtime, comparison harness, and module layout delivered earlier in Epic 4 (`docs/stories/4.3.build-docker-container.md:9-160`, `docs/brownfield-architecture.md:935-959`).  
- [x] Supports QA/PO readiness checks tied to MSC4 performance evidence (`docs/prd.md:299-305`).  
- [x] Maintains sanitized data handling and logging guardrails for PII masking (`docs/prd.md:252-257`, `docs/ops/logging.md:129-168`).

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-23 | 1.0 | Initial draft covering performance validation scope for Epic 4 | Bob (Scrum Master) |

## Dev Agent Record

